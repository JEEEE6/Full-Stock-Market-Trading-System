{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Images ===\n",
      " - placeholder.png\n",
      " - weapon_candycane\n",
      " - luxuryfinish_attachment.png\n",
      " - xtc_brick.png\n",
      " - oxy_brick.png\n",
      " - meth_brick.png\n",
      " - coke_seed.png\n",
      " - tomato_seed.png\n",
      " - crafting_table_bad.png\n",
      " - crafting_table_normal.png\n",
      " - crafting_table_good.png\n",
      " - crafting_general_station.png\n",
      " - highentropyalloyingot.png\n",
      " - weapon_dildo.png\n",
      " - explosivecharge.png\n",
      " - hydraulicspreader.png\n",
      "\n",
      "Total missing images: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Adjust these paths if necessary\n",
    "ITEMS_LUA_PATH = r\"C:\\FXServer\\txData\\QBCoreFramework_5784FC.base\\resources\\[qb]\\qb-core\\shared\\items.lua\"\n",
    "IMAGES_FOLDER = r\"C:\\FXServer\\txData\\QBCoreFramework_5784FC.base\\resources\\[qb]\\qb-inventory\\html\\images\"\n",
    "\n",
    "# Regex to capture the filename from lines like: image = 'weapon_bat.png',\n",
    "pattern = r\"image\\s*=\\s*'([^']+)'\"\n",
    "\n",
    "missing_images = []\n",
    "\n",
    "# 1. Read the entire items.lua\n",
    "with open(ITEMS_LUA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# 2. Find all \"image = 'filename.ext'\" occurrences\n",
    "matches = re.findall(pattern, content)\n",
    "\n",
    "# 3. For each image name, check if it exists in the images folder\n",
    "for image_name in matches:\n",
    "    image_path = os.path.join(IMAGES_FOLDER, image_name)\n",
    "    if not os.path.exists(image_path):\n",
    "        missing_images.append(image_name)\n",
    "\n",
    "# 4. Print the results\n",
    "print(\"\\n=== Missing Images ===\")\n",
    "if missing_images:\n",
    "    for img in missing_images:\n",
    "        print(f\" - {img}\")\n",
    "    print(f\"\\nTotal missing images: {len(missing_images)}\")\n",
    "else:\n",
    "    print(\"No missing images! All good.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = \"Data/PriceData\"\n",
    "SAMPLE_SIZE = 1000\n",
    "WEIGHTS = [0.75, 0.15, 0.1]  # 1d, 5d, 20d\n",
    "\n",
    "def safe_divide(a, b):\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=(b != 0) & (~np.isnan(b)))\n",
    "\n",
    "def safe_log(x):\n",
    "    return np.log(np.abs(x) + 1e-10) * np.sign(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.stats import spearmanr\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"Data/PriceData\"\n",
    "SAMPLE_SIZE = 1000\n",
    "WEIGHTS = [0.75, 0.15, 0.1]  # 1d, 5d, 20d\n",
    "\n",
    "def safe_divide(a, b):\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=(b != 0) & (~np.isnan(b)))\n",
    "\n",
    "def safe_log(x):\n",
    "    return np.log(np.abs(x) + 1e-10) * np.sign(x)\n",
    "\n",
    "def calculate_rolling_oos(signal, target, window=63):\n",
    "    \"\"\"Rolling out-of-sample validation\"\"\"\n",
    "    oos = []\n",
    "    if len(signal) <= window:\n",
    "        return np.nan\n",
    "    for i in range(len(signal)-window):\n",
    "        train = signal.iloc[i:i+window]\n",
    "        test = signal.iloc[i+window]\n",
    "        if len(train.dropna()) > 10 and not np.isnan(test):\n",
    "            corr = spearmanr(test, target.iloc[i+window]).correlation\n",
    "            oos.append(corr if not np.isnan(corr) else 0)\n",
    "    return np.nanmean(oos) if oos else np.nan\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_dynamic_retracement_indicator(df, \n",
    "                                         lookback=60, \n",
    "                                         ret_quantile=0.80, \n",
    "                                         vol_quantile=0.40, \n",
    "                                         price_col='Close', \n",
    "                                         volume_col='Volume'):\n",
    "    \"\"\"\n",
    "    Create a fully adaptive 'Dynamic Retracement Indicator' (DRI) that flags \n",
    "    contrarian signals when the recent price move is extreme, yet volume \n",
    "    remains low (relative to recent history). \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain 'Close' and 'Volume' columns (or user-specified).\n",
    "    lookback : int\n",
    "        Rolling window size (in bars) to calculate adaptive thresholds.\n",
    "    ret_quantile : float\n",
    "        Quantile to define an \"extreme\" absolute return over the short window.\n",
    "        E.g., 0.80 means we pick the 80th percentile of absolute returns from \n",
    "        the last 'lookback' bars as a threshold.\n",
    "    vol_quantile : float\n",
    "        Quantile to define \"low\" volume environment from the last 'lookback' bars.\n",
    "        E.g., 0.40 means if today's volume is below the 40th percentile of \n",
    "        past 'lookback' volumes, it is considered \"low.\"\n",
    "    price_col : str\n",
    "        The name of the column in `df` containing prices.\n",
    "    volume_col : str\n",
    "        The name of the column in `df` containing volumes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        A copy of the input with an added 'DRI' column. This is the *signal* \n",
    "        (positive = long, negative = short, NaN = no signal).\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - We compute a short-window return (3-day by default). Adjust if desired.\n",
    "    - We do NOT incorporate any future returns inside the condition \n",
    "      (to avoid data leakage). \n",
    "    - Performance measurement vs. future returns can be handled afterward \n",
    "      (e.g. using df['Close'].pct_change(5).shift(-5) or some custom target).\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1) Compute a short-term return (3-day by default).\n",
    "    #    Feel free to parameterize short_window if you'd like.\n",
    "    df['ret_3d'] = df[price_col].pct_change(3)\n",
    "    \n",
    "    # 2) Create a rolling \"extreme move\" threshold based on ABS returns:\n",
    "    #    e.g., the 80th percentile of the absolute 3-day returns in the last `lookback` bars.\n",
    "    #    For each row, we only look at the *past* data, so shift=1 or min_periods can help.\n",
    "    rolling_abs_ret = df['ret_3d'].abs().rolling(lookback, min_periods=10)\n",
    "    df['abs_ret_threshold'] = rolling_abs_ret.quantile(ret_quantile)\n",
    "    \n",
    "    # 3) Create a rolling \"low volume\" threshold from the last `lookback` bars.\n",
    "    rolling_volume = df[volume_col].rolling(lookback, min_periods=10)\n",
    "    df['volume_threshold'] = rolling_volume.quantile(vol_quantile)\n",
    "    \n",
    "    # 4) Condition for \"extreme move\" = abs(ret_3d) > abs_ret_threshold \n",
    "    #    AND \"low volume\" = Volume < volume_threshold\n",
    "    #    Contrarian signal = - sign(ret_3d)\n",
    "    condition = (\n",
    "        (df['ret_3d'].abs() > df['abs_ret_threshold']) & \n",
    "        (df[volume_col] < df['volume_threshold'])\n",
    "    )\n",
    "    \n",
    "    df['DRI'] = np.where(condition, -np.sign(df['ret_3d']), np.nan)\n",
    "    \n",
    "    # Optionally, you could scale the DRI by how extreme the move is \n",
    "    # above the threshold, for example:\n",
    "    #\n",
    "    #   distance_factor = (df['ret_3d'].abs() - df['abs_ret_threshold']) / df['abs_ret_threshold']\n",
    "    #   df['DRI'] = np.where(condition, -np.sign(df['ret_3d']) * (1 + distance_factor.clip(lower=0)), np.nan)\n",
    "    #\n",
    "    # But keep it simple unless you want a magnitude-based signal.\n",
    "    \n",
    "    # 5) Clean up extra columns if desired:\n",
    "    df.drop(['ret_3d', 'abs_ret_threshold', 'volume_threshold'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_ta_features(df):\n",
    "\n",
    "    #use the dynamic retracement indicator \n",
    "    df = create_dynamic_retracement_indicator(df)\n",
    "\n",
    "\n",
    "    # Market Regime Analysis\n",
    "    try:\n",
    "        volatility = df['Close'].rolling(20).std() / df['Close']\n",
    "        bins = [float('-inf')] + list(np.percentile(volatility.dropna(), [33.33, 66.67])) + [float('inf')]\n",
    "        df['Volatility_Regime'] = pd.cut(\n",
    "            volatility,\n",
    "            bins=bins,\n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            # Fallback method 1\n",
    "            df['Volatility_Regime'] = pd.qcut(\n",
    "                df['Close'].rolling(20).std() / df['Close'],\n",
    "                q=3,\n",
    "                labels=['Low', 'Medium', 'High'],\n",
    "                duplicates='drop'\n",
    "            ).fillna('Medium')\n",
    "        except:\n",
    "            # Ultimate fallback\n",
    "            df['Volatility_Regime'] = 'Medium'\n",
    "    \n",
    "\n",
    "    # Volume Profile Analysis - Fixed Version\n",
    "    df['Volume_Pctl'] = (\n",
    "        df['Volume'].rolling(20)\n",
    "        .rank(pct=True)\n",
    "        .mul(4)  # Changed from 5 to 4 since rank returns 0-1\n",
    "        .add(1)  # Add 1 to get range 1-5\n",
    "        .fillna(method='ffill')\n",
    "        .fillna(0)  # Handle any remaining NaNs\n",
    "        .round()  # Round to nearest integer\n",
    "        .astype('Int64')  # Use pandas nullable integer type instead of standard int\n",
    "    )\n",
    "\n",
    "    # Block Trade Detection\n",
    "    df['Block_Trade'] = np.where(\n",
    "        (df['Volume'] > df['Volume'].rolling(20).mean() * 2) &\n",
    "        (df['High'] - df['Low'] < df['Close'] * 0.001) &\n",
    "        (df['Close'] > df['Open']),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Existing Core Features\n",
    "    lookback = 14\n",
    "    df['MA_14'] = df['Close'].rolling(lookback).mean()\n",
    "    df['STD_14'] = df['Close'].rolling(lookback).std()\n",
    "    df['Volume_Rank'] = df['Volume'].rolling(lookback).rank(pct=True)\n",
    "    \n",
    "    # Enhanced Price Deviation\n",
    "    price_deviation = (df['Close'] - df['MA_14']) / df['STD_14']\n",
    "    volume_impact = np.log1p(df['Volume'] / df['Volume'].rolling(20).mean())\n",
    "    df['Enhanced_Deviation'] = price_deviation * volume_impact\n",
    "    \n",
    "    # Conditional Logic Components\n",
    "    df['MA_3'] = df['Close'].rolling(3).mean().pct_change(3)\n",
    "    \n",
    "    # Recent volatility measurement\n",
    "    df['Max_5D_Move'] = df['Close'].pct_change().abs().rolling(5).max()\n",
    "    \n",
    "    # Core Indicator Logic - Single Implementation\n",
    "    df['Breakout_Anticipator'] = np.nan\n",
    "    mask = (\n",
    "        (price_deviation.abs() > 1) & \n",
    "        (df['Volume_Rank'] > 0.9) & \n",
    "        (df['Volume'] > 1e5)  # Absolute volume filter\n",
    "    )\n",
    "    df.loc[mask, 'Breakout_Anticipator'] = (\n",
    "        np.sign(df['MA_3']) * \n",
    "        df['Max_5D_Move'] * \n",
    "        (1 + price_deviation.abs()/2)\n",
    "    )\n",
    "    \n",
    "    # Mean Reversion Detector\n",
    "    df['Reversion_Signal'] = np.nan\n",
    "    rev_mask = (\n",
    "        (price_deviation.abs() > 2) & \n",
    "        (df['Volume'] > df['Volume'].rolling(20).median())\n",
    "    )\n",
    "    df.loc[rev_mask, 'Reversion_Signal'] = (\n",
    "        -np.sign(price_deviation) * \n",
    "        df['Max_5D_Move'] * \n",
    "        np.log1p(df['Volume']/1e6)\n",
    "    )\n",
    "    \n",
    "    # Trend Persistence Enhancer\n",
    "    df['Momentum_Thrust'] = np.nan\n",
    "    thrust_mask = (\n",
    "        (df['Close'] > df['High'].rolling(5).max()) &\n",
    "        (df['Volume'] > df['Volume'].rolling(20).quantile(0.8))\n",
    "    )\n",
    "    df.loc[thrust_mask, 'Momentum_Thrust'] = (\n",
    "        df['Close'].pct_change(3) * \n",
    "        (df['Volume'] / df['Volume'].rolling(20).mean())\n",
    "    )\n",
    "    \n",
    "    # Range and CEM\n",
    "    window = 14\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "    range_ma = df['Range'].rolling(window).mean()\n",
    "    df['CEM'] = np.where(\n",
    "        (df['Range'] > 1.5*range_ma) & (df['Close'] > df['Open']),\n",
    "        df['Close'].pct_change(3).shift(-3),\n",
    "        np.where(\n",
    "            (df['Range'] > 1.5*range_ma) & (df['Close'] < df['Open']),\n",
    "            -df['Close'].pct_change(3).shift(-3),\n",
    "            np.nan\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Liquidity Vacuum Indicator\n",
    "    vol_ma = df['Volume'].rolling(20).mean()\n",
    "    df['LVI'] = np.where(\n",
    "        (df['Volume'] < 0.7*vol_ma) & \n",
    "        (df['Close'].rolling(5).std() < 0.015*df['Close']),\n",
    "        (df['Close'].rolling(20).quantile(0.9) - df['Close']) / df['Close'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Paradoxical Reversal Signal\n",
    "    ret_3d = df['Close'].pct_change(3)\n",
    "    df['PRS'] = np.where(\n",
    "        (ret_3d.abs() > 0.15) & \n",
    "        (df['Volume'] < df['Volume'].rolling(10).quantile(0.4)),\n",
    "        -np.sign(ret_3d) * df['Close'].pct_change(5).shift(-5),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Volume-Confirmation Divergence\n",
    "    price_high = df['Close'].rolling(14).max()\n",
    "    vol_high = df['Volume'].rolling(14).max()\n",
    "    df['VCD'] = np.where(\n",
    "        (df['Close'] == price_high) & (df['Volume'] < 0.8*vol_high),\n",
    "        -df['Close'].pct_change(5).shift(-5),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Fractal Efficiency Ratio\n",
    "    move = df['Close'].diff(5).abs()\n",
    "    volatility = df['High'].rolling(5).max() - df['Low'].rolling(5).min()\n",
    "    df['FER'] = np.where(\n",
    "        (move/volatility > 0.7) & (df['Volume'] > df['Volume'].rolling(20).mean()),\n",
    "        move * np.sign(df['Close'].diff(5)) / df['Close'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Naive Benchmark\n",
    "    df['Naive_Benchmark'] = df['Close'].pct_change()\n",
    "    \n",
    "    # Apply optimizations first to create VCD_Directional\n",
    "\n",
    "\n",
    "    df = optimize_indicators(df)\n",
    "    \n",
    "    # Create SMFI after optimize_indicators but before Smart_Money_Flow\n",
    "    df['SMFI'] = np.where(\n",
    "        (df['Block_Trade'] == 1) &\n",
    "        (df['Volume_Pctl'] >= 4),\n",
    "        df['Close'].pct_change() * volume_impact,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Now create Smart_Money_Flow after SMFI exists\n",
    "    df['Smart_Money_Flow'] = (\n",
    "        0.5 * df['VCD_Directional'].fillna(0) +\n",
    "        0.3 * df['SMFI'].fillna(0) +\n",
    "        0.2 * df['Block_Trade'].fillna(0)\n",
    "    ).replace(0, np.nan)\n",
    "    \n",
    "    # Volatility Regime Adjusted Signals\n",
    "    for regime in ['Low', 'Medium', 'High']:\n",
    "        mask = df['Volatility_Regime'] == regime\n",
    "        df[f'VCD_Regime_{regime}'] = np.where(\n",
    "            mask,\n",
    "            df['VCD_Directional'] * [0.8, 1.0, 1.5][['Low', 'Medium', 'High'].index(regime)],\n",
    "            np.nan\n",
    "        )\n",
    "    \n",
    "    # Adaptive Signal Scaling - Move this here from optimize_indicators\n",
    "    # Volatility Regime Adjusted Signals\n",
    "    regime_multipliers = {'Low': 0.8, 'Medium': 1.0, 'High': 1.5}\n",
    "    for regime in ['Low', 'Medium', 'High']:\n",
    "        df[f'VCD_Regime_{regime}'] = np.where(\n",
    "            df['Volatility_Regime'] == regime,\n",
    "            df['VCD_Directional'] * regime_multipliers[regime],\n",
    "            np.nan\n",
    "        )\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(columns=['MA_14', 'STD_14', 'Volume_Rank', 'MA_3', 'Max_5D_Move', 'Range'])\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "def optimize_indicators(df):\n",
    "    # Original inverse transformations\n",
    "    df['Reversion_Signal_Inverse'] = -df['Reversion_Signal']\n",
    "    df['FER_Inverse'] = -df['FER']\n",
    "    \n",
    "    # Original indicators from previous version (moved up)\n",
    "    df['VCD_Directional'] = np.where(df['VCD'] < 0, -df['VCD'], np.nan)\n",
    "    \n",
    "    # Enhanced VCD Directional with relaxed thresholds\n",
    "    df['VCD_Directional_v2'] = np.where(\n",
    "        ((df['Close'] - df['MA_14']).abs()/df['STD_14'] > 1.5) &\n",
    "        (df['Volume'] > df['Volume'].rolling(20).median()),\n",
    "        -np.sign(df['Close'] - df['MA_14']) * df['STD_14']/df['Close'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Enhanced PRS with volatility scaling\n",
    "    prs_threshold = df['PRS'].quantile(0.6)\n",
    "    df['PRS_Enhanced_v2'] = np.where(\n",
    "        df['PRS'] > prs_threshold,\n",
    "        df['PRS'] * (1 + df['Close'].pct_change().rolling(5).std()*100),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    df['PRS_Enhanced'] = np.where(\n",
    "        df['PRS'] > df['PRS'].quantile(0.7), \n",
    "        df['PRS'] * 1.5,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Earnings Thrust indicators\n",
    "    earnings_window = 5\n",
    "    df['Earnings_Thrust'] = np.where(\n",
    "        df['Volume'].rolling(earnings_window).std() > 2*df['Volume'].rolling(20).std(),\n",
    "        df['Close'].pct_change(earnings_window).shift(-earnings_window) * \n",
    "        np.log1p(df['Volume']/1e6),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    df['Earnings_Thrust_v2'] = np.where(\n",
    "        df['Earnings_Thrust'].notna(),\n",
    "        df['Earnings_Thrust'] * (1 + df['Volume'].pct_change().rolling(5).std()),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # LVI-Volatility Composite\n",
    "    df['LVI_Vol_Composite'] = df['LVI'] * np.sqrt(\n",
    "        df['Volume'].rolling(20).mean()/df['Volume']\n",
    "    )\n",
    "    \n",
    "    # Breakout Anticipator Inverse with volume decay\n",
    "    df['Breakout_Anticipator_Inverse'] = np.where(\n",
    "        df['Breakout_Anticipator'].notna(),\n",
    "        -df['Breakout_Anticipator'] * (df['Volume']/df['Volume'].rolling(20).mean()),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Regime-Filtered Naive Benchmark\n",
    "    vol_filter = df['Close'].rolling(20).std() < 0.015*df['Close']\n",
    "    df['Naive_Benchmark_v2'] = np.where(\n",
    "        vol_filter,\n",
    "        df['Naive_Benchmark'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Weighted Composite Alpha\n",
    "    df['Composite_Alpha'] = (\n",
    "        0.4 * df['VCD_Directional_v2'].fillna(0) +\n",
    "        0.3 * df['PRS_Enhanced_v2'].fillna(0) +\n",
    "        0.2 * df['Earnings_Thrust_v2'].fillna(0) +\n",
    "        0.1 * df['LVI_Vol_Composite'].fillna(0)\n",
    "    ).replace(0, np.nan)\n",
    "    \n",
    "    df['LVI_PRIS_Combo'] = np.where(\n",
    "        df['LVI'].notna() & df['PRS'].notna(),\n",
    "        (df['LVI'] * 0.6 + df['PRS'] * 0.4) * df['Volume'].pct_change(3),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    df['Gamma_Squeeze'] = np.where(\n",
    "        (df['Close'].rolling(5).std() > 0.1*df['Close']) &\n",
    "        (df['Volume'].rolling(5).mean() > 2*df['Volume'].rolling(20).mean()),\n",
    "        df['High'].rolling(3).max() / df['Low'].rolling(3).min() - 1,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    df['Inst_Flow'] = np.where(\n",
    "        (df['Volume'] > 1e6) &\n",
    "        (df['Close'] == df['High']) &\n",
    "        (df['Volume'] > df['Open'] * 1000),\n",
    "        df['Volume'].rolling(10).mean().pct_change(3),\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(feature, target, returns):\n",
    "    valid_mask = feature.notna() & target.notna()\n",
    "    if valid_mask.sum() < 10:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Handle categorical data\n",
    "    if pd.api.types.is_categorical_dtype(feature):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Convert to float if needed\n",
    "    feature_vals = pd.to_numeric(feature, errors='coerce').dropna()\n",
    "    target_vals = target[valid_mask]\n",
    "    \n",
    "    ic = spearmanr(feature_vals, target_vals).correlation\n",
    "    \n",
    "    # Improved profit factor calculation\n",
    "    direction = np.sign(feature_vals)\n",
    "    realized_returns = returns.loc[feature_vals.index] * direction\n",
    "    pf = realized_returns[realized_returns > 0].sum() / abs(realized_returns[realized_returns < 0].sum())\n",
    "    \n",
    "    if len(realized_returns) < 30:  # Minimum sample threshold\n",
    "        return ic, np.nan\n",
    "    \n",
    "    gains = realized_returns[realized_returns > 0]\n",
    "    losses = realized_returns[realized_returns < 0]\n",
    "    \n",
    "    if len(losses) == 0:\n",
    "        pf = gains.sum() / 1e-6  # Regularized PF\n",
    "    else:\n",
    "        pf = gains.sum() / abs(losses.sum())\n",
    "    \n",
    "    return ic, pf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_outlier(indicator_name, results_df):\n",
    "    outlier = results_df[results_df['Indicator'] == indicator_name]\n",
    "    print(f\"\\nOutlier Diagnostic - {indicator_name}:\")\n",
    "    print(f\"Signal Frequency: {outlier['Coverage'].values[0]:.2%}\")\n",
    "    print(f\"Return Distribution: {outlier['IC'].values[0]:.2f} IC\")\n",
    "    print(f\"Risk/Reward Profile: {outlier['ProfitFactor'].values[0]:.1f} PF\")\n",
    "    \n",
    "    if outlier['Coverage'].values[0] < 0.05:\n",
    "        print(\"⚠️ Warning: Low signal frequency - results likely unreliable\")\n",
    "    if outlier['ProfitFactor'].values[0] > 10:\n",
    "        print(\"⚠️ Extreme PF suggests overfitting or small sample size\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_indicators(df):\n",
    "    results = []\n",
    "    returns = df['Close'].pct_change().shift(-1)\n",
    "    \n",
    "    # Only evaluate numerical features\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    indicators = [c for c in numeric_cols \n",
    "                 if c not in ['Date', 'Ticker', 'Target', 'Open', \n",
    "                             'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "    \n",
    "    for col in indicators:\n",
    "        ic, pf = calculate_metrics(df[col], df['Target'], returns)\n",
    "        results.append({\n",
    "            'Indicator': col,\n",
    "            'IC': ic,\n",
    "            'ProfitFactor': pf,\n",
    "            'Coverage': df[col].notna().mean()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc # Garbage Collector\n",
    "\n",
    "# Block 2: Simplified Analysis and Visualization\n",
    "def process_single_file(file_path, weights):\n",
    "    \"\"\"Process one file and return its indicator metrics\"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        ticker = file_path.stem\n",
    "        \n",
    "        # Calculate target\n",
    "        df = calculate_target(df)\n",
    "        \n",
    "        # Create features\n",
    "        df = create_ta_features(df)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = []\n",
    "        returns = df['Close'].pct_change().shift(-1)\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        indicators = [c for c in numeric_cols if c not in ['Target', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "        \n",
    "        for col in indicators:\n",
    "            ic, pf = calculate_metrics(df[col], df['Target'], returns)\n",
    "            coverage = df[col].notna().mean()\n",
    "            results.append({\n",
    "                'Ticker': ticker,\n",
    "                'Indicator': col,\n",
    "                'IC': ic,\n",
    "                'ProfitFactor': pf,\n",
    "                'Coverage': coverage\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def run_full_analysis(data_path=DATA_PATH, sample_size=10, weights=WEIGHTS):\n",
    "    \"\"\"Process files individually with memory cleanup\"\"\"\n",
    "    all_files = [f for f in Path(data_path).glob(\"*.parquet\")]\n",
    "    selected_files = np.random.choice(all_files, min(sample_size, len(all_files)), replace=False)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process files one by one with progress bar\n",
    "    for file_path in tqdm(selected_files, desc=\"Processing Files\"):\n",
    "        file_results = process_single_file(file_path, weights)\n",
    "        if not file_results.empty:\n",
    "            all_results.append(file_results)\n",
    "        \n",
    "        # Explicit memory cleanup\n",
    "        del file_results\n",
    "        gc.collect()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Combine results\n",
    "    results_df = pd.concat(all_results).groupby('Indicator').agg({\n",
    "        'IC': 'mean',\n",
    "        'ProfitFactor': 'mean',\n",
    "        'Coverage': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = px.scatter(results_df,\n",
    "                    x='IC',\n",
    "                    y='ProfitFactor',\n",
    "                    text='Indicator',\n",
    "                    title=f'Indicator Performance (n={len(selected_files)} files)',\n",
    "                    labels={'IC': 'Information Coefficient', 'ProfitFactor': 'Profit Factor'},\n",
    "                    hover_data=['Indicator', 'IC', 'ProfitFactor', 'Coverage'])\n",
    "    \n",
    "    # Add performance thresholds\n",
    "    fig.add_shape(type=\"line\", x0=0.15, y0=0, x1=0.15, y1=3,\n",
    "                line=dict(color=\"red\", width=2, dash=\"dot\"))\n",
    "    fig.add_shape(type=\"line\", x0=-0.5, y0=1.2, x1=0.5, y1=1.2,\n",
    "                line=dict(color=\"red\", width=2, dash=\"dot\"))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis_range=[-0.2, 0.5],\n",
    "        yaxis_range=[0.8, 3],\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTop Performers:\")\n",
    "    print(results_df.sort_values('ProfitFactor', ascending=False).head(10).to_string(index=False))\n",
    "    \n",
    "\n",
    "    #show the graph \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    return results_df\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Updated target calculation\n",
    "def calculate_target(df):\n",
    "    close = df['Adj Close']\n",
    "    \n",
    "    # Weighted returns with proper alignment\n",
    "    df['Target'] = (0.75 * close.pct_change().shift(-1).fillna(0) +\n",
    "                  0.20 * close.pct_change(5).shift(-5).fillna(0) +\n",
    "                  0.05 * close.pct_change(20).shift(-20).fillna(0))\n",
    "    return df\n",
    "\n",
    "results_df = run_full_analysis(sample_size=5000)  # Now handles 500 files easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date    Open    High     Low   Close       Volume  Average  BarCount  \\\n",
      "0 2024-02-27  180.60  183.93  179.56  183.06   44885592.0  181.837    191479   \n",
      "1 2024-02-28  183.30  183.47  180.13  181.22   40521639.0  181.163    163556   \n",
      "2 2024-02-29  180.89  182.57  179.53  180.63  105396081.0  180.689    266290   \n",
      "3 2024-03-01  180.60  181.30  177.38  179.00   59067834.0  179.025    249051   \n",
      "4 2024-03-04  179.93  179.93  173.79  174.60   63591036.0  174.882    286768   \n",
      "\n",
      "   BID_Open  BID_High  ...  MIDPOINT_BarCount  BID_ASK_Open  BID_ASK_High  \\\n",
      "0    179.88    183.92  ...                 -1        181.57        189.10   \n",
      "1    181.38    183.09  ...                 -1        181.53        183.31   \n",
      "2    180.01    182.56  ...                 -1        180.64        182.57   \n",
      "3    179.75    181.21  ...                 -1        179.23        181.58   \n",
      "4    178.33    179.33  ...                 -1        175.80        180.44   \n",
      "\n",
      "   BID_ASK_Low  BID_ASK_Close  BID_ASK_Volume  BID_ASK_Average  \\\n",
      "0       179.88         181.61            -1.0             -1.0   \n",
      "1       180.13         181.57            -1.0             -1.0   \n",
      "2       179.53         180.68            -1.0             -1.0   \n",
      "3       177.38         179.26            -1.0             -1.0   \n",
      "4       173.79         175.83            -1.0             -1.0   \n",
      "\n",
      "   BID_ASK_BarCount  Ticker  Timeframe  \n",
      "0                -1    AAPL      DAILY  \n",
      "1                -1    AAPL      DAILY  \n",
      "2                -1    AAPL      DAILY  \n",
      "3                -1    AAPL      DAILY  \n",
      "4                -1    AAPL      DAILY  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "##read this file here Data\\PriceData\\AAPL\\AAPL_DAILY.parquet and print the head \n",
    "df = pd.read_parquet('Data/PriceData/AAPL/AAPL_DAILY.parquet')\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
